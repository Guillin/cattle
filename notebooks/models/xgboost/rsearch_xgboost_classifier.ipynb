{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST - RANDOM SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn import datasets\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/train_test/'\n",
    "SEED = 47\n",
    "NITER = 100\n",
    "CV = 5\n",
    "SCORE = 'balanced_accuracy'\n",
    "handlingnull = False\n",
    "NJOBS = 5\n",
    "USEGPU = True\n",
    "NCLASS = 3 # number class to predict (if bivar set 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.load(DATAPATH+'X_features_clusters_001_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load(DATAPATH+'y_features_clusters_001_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1225, 1275)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1225,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a DMatrix and handling Null values\n",
    "if handlingnull:\n",
    "    #train_features[np.isnan(train_features)] = -9999\n",
    "    xgtrain = xgb.DMatrix(train_features, train_labels, missing=-9999)\n",
    "else:\n",
    "    xgtrain = xgb.DMatrix(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== General Parameters ======= #\n",
    "\n",
    "# Select the type of model to run at each iteration. gbtree or gblinear.\n",
    "booster = 'gbtree'\n",
    "\n",
    "\n",
    "# ======== Booster Parameters ======== # \n",
    "\n",
    "# Analogous to learning rate in GBM. \n",
    "# Typical final values to be used: 0.01-0.2\n",
    "eta = [0.01] \n",
    "\n",
    "# Defines the minimum sum of weights of all observations required in a child.\n",
    "min_child_weight = [i for i in range(1,10,2)]\n",
    "\n",
    "# The maximum depth of a tree\n",
    "max_depth = [i for i in range(3,10,2)] \n",
    "\n",
    "# A node is split only when the resulting split gives a positive reduction in the loss function. \n",
    "# Gamma specifies the minimum loss reduction required to make a split.\n",
    "gamma = [i/10.0 for i in range(0,5)]\n",
    "\n",
    "# Denotes the fraction of observations to be randomly samples for each tree.\n",
    "subsample = [i/10.0 for i in range(6,10)]\n",
    "\n",
    "# Denotes the fraction of columns to be randomly samples for each tree.\n",
    "colsample_bytree = [i/10.0 for i in range(6,10)]\n",
    "\n",
    "# L2 regularization term on weights (analogous to Ridge regression)\n",
    "reg_lambda = [i/10.0 for i in range(4,10)]\n",
    "\n",
    "# L1 regularization term on weight (analogous to Lasso regression)\n",
    "reg_alpha = [0, 0.001, 0.005, 0.01, 0.05]\n",
    "\n",
    "# Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "# A typical value to consider: sum(negative instances) / sum(positive instances)scale_pos_weight = 1\n",
    "scale_pos_weight = [5, 1, 2] #int((len(train_labels) - np.sum(train_labels))/np.sum(train_labels))\n",
    "\n",
    "\n",
    "# Learning Task Parameters\n",
    "\n",
    "# This defines the loss function to be minimized. \n",
    "# - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "# - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "#   you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "# - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "objective  = 'multi:softprob'\n",
    "\n",
    "\n",
    "# The metric to be used for validation data.\n",
    "# - rmse – root mean square error\n",
    "# - mae – mean absolute error\n",
    "# - logloss – negative log-likelihood\n",
    "# - error – Binary classification error rate (0.5 threshold)\n",
    "# - merror – Multiclass classification error rate\n",
    "# - mlogloss – Multiclass logloss\n",
    "# - auc: Area under the curve\n",
    "eval_metric = 'mlogloss'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[xgboost params](https://xgboost.readthedocs.io/en/latest/python/python_api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random grid\n",
    "random_grid = {\n",
    "    'learning_rate' : eta,\n",
    "    'min_child_weight' : min_child_weight,\n",
    "    'max_depth' : max_depth,\n",
    "    'gamma': gamma,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree' : colsample_bytree,\n",
    "    'reg_lambda' : reg_lambda,\n",
    "    'reg_alpha' : reg_alpha,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': [0.01],\n",
       " 'min_child_weight': [1, 3, 5, 7, 9],\n",
       " 'max_depth': [3, 5, 7, 9],\n",
       " 'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
       " 'subsample': [0.6, 0.7, 0.8, 0.9],\n",
       " 'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
       " 'reg_lambda': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
       " 'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find num boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_rounds = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=XGBClassifier(seed=SEED, booster=booster, objective=objective,  scale_pos_weight = scale_pos_weight, nthread=NJOBS)\n",
    "xgb_param = model.get_xgb_params()\n",
    "xgb_param['num_class'] = NCLASS\n",
    "\n",
    "if USEGPU:\n",
    "    xgb_param['tree_method'] = 'gpu_hist'\n",
    "    xgb_param['gpu_id'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvresult = xgb.cv(xgb_param, xgtrain, \n",
    "                  num_boost_round = 1000, \n",
    "                  nfold = CV, \n",
    "                  metrics = eval_metric, \n",
    "                  early_stopping_rounds = early_stopping_rounds,\n",
    "                  #num_class= NCLASS,\n",
    "                  seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = cvresult.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of boosters:  58\n"
     ]
    }
   ],
   "source": [
    "print(\"Best number of boosters: \", n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "model = XGBClassifier(n_estimators=n_estimators, scale_pos_weight=scale_pos_weight,  objective=objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USEGPU:\n",
    "    model.set_params(gpu_id = 0)\n",
    "    model.set_params(tree_method='gpu_hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search of parameters, using CV fold cross validation, \n",
    "# search across NITER different combinations, and use all available cores\n",
    "xgboost_rsearch = RandomizedSearchCV(estimator = model, param_distributions = random_grid, scoring=SCORE, n_iter = NITER, cv = CV, verbose=2, random_state=SEED, n_jobs = NJOBS)# Fit the random search model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "/opt/anaconda3/envs/xgbgpuenv/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=5)]: Done 152 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=5)]: Done 355 tasks      | elapsed: 30.1min\n",
      "[Parallel(n_jobs=5)]: Done 500 out of 500 | elapsed: 39.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 2399.15 seconds for 100 candidates parameter settings.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "xgboost_rsearch.fit(train_features, train_labels)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), NITER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/xgbgpuenv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/anaconda3/envs/xgbgpuenv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/anaconda3/envs/xgbgpuenv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/anaconda3/envs/xgbgpuenv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/anaconda3/envs/xgbgpuenv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/anaconda3/envs/xgbgpuenv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/anaconda3/envs/xgbgpuenv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(xgboost_rsearch.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.to_csv('output/results/rsearch_xgboost_classifier_d' + str(datetime.now().date()) + '.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.9, gamma=0.1, gpu_id=0,\n",
       "       learning_rate=0.01, max_delta_step=0, max_depth=9,\n",
       "       min_child_weight=5, missing=None, n_estimators=58, n_jobs=1,\n",
       "       nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0.05, reg_lambda=0.4, scale_pos_weight=[5, 1, 2],\n",
       "       seed=None, silent=None, subsample=0.7, tree_method='gpu_hist',\n",
       "       verbosity=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_rsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.7,\n",
       " 'reg_lambda': 0.4,\n",
       " 'reg_alpha': 0.05,\n",
       " 'min_child_weight': 5,\n",
       " 'max_depth': 9,\n",
       " 'learning_rate': 0.01,\n",
       " 'gamma': 0.1,\n",
       " 'colsample_bytree': 0.9}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_rsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced_accuracy  :  0.7563362902305817\n"
     ]
    }
   ],
   "source": [
    "print(SCORE,' : ', xgboost_rsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('output/hyperparameters/rseach_xgboost_classifier_bestparams_d' + str(datetime.now().date()) + '.npy', xgboost_rsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('output/results/rseach_xgboost_classifier_best_estimator_d' + str(datetime.now().date()) + '.npy', xgboost_rsearch.best_estimator_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Xgboost GPU(env)",
   "language": "python",
   "name": "xgbgpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
