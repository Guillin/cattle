{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u22w3BFiOveA"
   },
   "source": [
    "# Preparación del entorno\n",
    "El primer paso es importar las librerías necesarias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9X5VYFveyhvp"
   },
   "source": [
    "* En el caso de que la librería no esté disponible en el entorno virtual, podemos instalarla agregando la línea:\n",
    "\n",
    "```bash\n",
    "!pip3 install hiddenlayer\n",
    "```\n",
    "\n",
    "* Cualquier línea que comience con un signo de exclamación será ejecutada por el sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_0Ufl9TfW_up"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as tr\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile as zf\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from scipy.io import wavfile as wv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3mrVb-RDVUU"
   },
   "source": [
    "# Lectura de los archivos wav\n",
    "Creamos una clase que nos permitirá manejar el dataset de forma más cómoda en las próximas etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpFd697CZ9re"
   },
   "outputs": [],
   "source": [
    "class WaveDataset(Dataset):\n",
    "    def __init__(self, data_folder, annotation_file):\n",
    "        self.data_folder = data_folder\n",
    "        ds = pd.read_csv(annotation_file)\n",
    "        self.filenames = list(ds['filename'])\n",
    "        if 'label' in ds.columns:\n",
    "            self.labels = list(ds['label'])\n",
    "        else:\n",
    "            self.labels = [-1 for i in range(len(self.filenames))]\n",
    "        self.cache = {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.labels))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index in self.cache:\n",
    "            data, label = self.cache[index]\n",
    "        else:\n",
    "            fname = os.path.join(self.data_folder, \"%04d.wav\" % self.filenames[index])\n",
    "            _, data = wv.read(fname)\n",
    "            label = self.labels[index]\n",
    "            self.cache[index] = (data, label)\n",
    "        return tr.Tensor(data), tr.LongTensor([label])\n",
    "    \n",
    "dataset = WaveDataset(\"data/raw/\", \"data/raw/train_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ff1zx7-NYkXg"
   },
   "source": [
    "# Definición el modelo neuronal\n",
    "Definimos una red convolucional 1D con Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ar6RG3T0Yr0X"
   },
   "outputs": [],
   "source": [
    "class CowCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CowCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.Conv1d(1, 4, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.AvgPool1d(2),\n",
    "            nn.Conv1d(4, 8, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.AvgPool1d(2),\n",
    "            nn.Conv1d(8, 16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.AvgPool1d(2),\n",
    "            nn.Conv1d(16, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1))\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 3))\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 17640)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 32)\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7C1zl41fyc-"
   },
   "source": [
    "# Entrenamiento del modelo\n",
    "El siguiente paso es entrenar el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8R2Quo0Pyr2B"
   },
   "source": [
    "* Para utilizar GPU es importante activar el uso de esta en (*Edit -> Notebook Settings -> Hardware Accelerator*)\n",
    "* La primera época de entrenamiento puede tardar bastante dado que se deben cargar todos los datos en memoria. Las siguientes épocas deberían tardar un par de segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "E1DXziC1fxmg",
    "outputId": "d0c04ff2-3c9b-4614-96d3-469c4fb4da2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.763771\t Valid acc: 0.481953\n",
      "Train loss: 0.704779\t Valid acc: 0.600480\n",
      "Train loss: 0.671493\t Valid acc: 0.602321\n",
      "Train loss: 0.690069\t Valid acc: 0.607283\n",
      "Train loss: 0.671901\t Valid acc: 0.633814\n",
      "Train loss: 0.650565\t Valid acc: 0.572589\n",
      "Train loss: 0.659615\t Valid acc: 0.638896\n",
      "Train loss: 0.635902\t Valid acc: 0.605742\n",
      "Train loss: 0.650699\t Valid acc: 0.596539\n",
      "Train loss: 0.666217\t Valid acc: 0.642137\n",
      "Train loss: 0.625074\t Valid acc: 0.626331\n",
      "Train loss: 0.661522\t Valid acc: 0.613405\n",
      "Train loss: 0.608383\t Valid acc: 0.635834\n",
      "Train loss: 0.618037\t Valid acc: 0.604382\n",
      "Train loss: 0.600333\t Valid acc: 0.619708\n",
      "Train loss: 0.656815\t Valid acc: 0.623249\n",
      "Train loss: 0.621264\t Valid acc: 0.678351\n",
      "Train loss: 0.621647\t Valid acc: 0.690916\n",
      "Train loss: 0.587013\t Valid acc: 0.627191\n",
      "Train loss: 0.615848\t Valid acc: 0.651821\n",
      "Train loss: 0.588750\t Valid acc: 0.646919\n",
      "Train loss: 0.605194\t Valid acc: 0.642997\n",
      "Train loss: 0.636377\t Valid acc: 0.661505\n",
      "Train loss: 0.606175\t Valid acc: 0.683994\n",
      "Train loss: 0.603738\t Valid acc: 0.732773\n",
      "Train loss: 0.574971\t Valid acc: 0.719168\n",
      "Train loss: 0.577937\t Valid acc: 0.696559\n",
      "Train loss: 0.603849\t Valid acc: 0.660144\n",
      "Train loss: 0.592947\t Valid acc: 0.716647\n",
      "Train loss: 0.598795\t Valid acc: 0.705062\n",
      "Train loss: 0.593263\t Valid acc: 0.665786\n",
      "Train loss: 0.580498\t Valid acc: 0.682453\n",
      "Train loss: 0.595886\t Valid acc: 0.704202\n",
      "Train loss: 0.588958\t Valid acc: 0.641757\n",
      "Train loss: 0.570906\t Valid acc: 0.677191\n",
      "Train loss: 0.556996\t Valid acc: 0.699800\n",
      "Train loss: 0.574639\t Valid acc: 0.677111\n",
      "Train loss: 0.551814\t Valid acc: 0.693177\n",
      "Train loss: 0.584526\t Valid acc: 0.714566\n",
      "Train loss: 0.538430\t Valid acc: 0.722589\n",
      "Train loss: 0.568505\t Valid acc: 0.730372\n",
      "Train loss: 0.526818\t Valid acc: 0.730072\n",
      "Train loss: 0.537470\t Valid acc: 0.748760\n",
      "Train loss: 0.551697\t Valid acc: 0.709284\n",
      "Train loss: 0.569331\t Valid acc: 0.713405\n",
      "Train loss: 0.562458\t Valid acc: 0.687055\n",
      "Train loss: 0.542842\t Valid acc: 0.719528\n",
      "Train loss: 0.546668\t Valid acc: 0.692137\n",
      "Train loss: 0.545551\t Valid acc: 0.654402\n",
      "Train loss: 0.576930\t Valid acc: 0.639056\n",
      "Train loss: 0.566006\t Valid acc: 0.723569\n",
      "Train loss: 0.583942\t Valid acc: 0.686014\n",
      "Train loss: 0.542916\t Valid acc: 0.685514\n",
      "Train loss: 0.550792\t Valid acc: 0.708123\n",
      "Train loss: 0.592523\t Valid acc: 0.764566\n",
      "Train loss: 0.568845\t Valid acc: 0.757583\n",
      "Train loss: 0.560774\t Valid acc: 0.704802\n",
      "Train loss: 0.599341\t Valid acc: 0.762725\n",
      "Train loss: 0.550097\t Valid acc: 0.701140\n",
      "Train loss: 0.553937\t Valid acc: 0.707083\n",
      "Train loss: 0.541178\t Valid acc: 0.702861\n",
      "Train loss: 0.548951\t Valid acc: 0.674050\n",
      "Train loss: 0.543870\t Valid acc: 0.723269\n",
      "Train loss: 0.542203\t Valid acc: 0.756603\n",
      "Train loss: 0.565236\t Valid acc: 0.702301\n",
      "Train loss: 0.622592\t Valid acc: 0.679872\n",
      "Train loss: 0.540027\t Valid acc: 0.720208\n",
      "Train loss: 0.558448\t Valid acc: 0.723269\n",
      "Train loss: 0.576969\t Valid acc: 0.696058\n",
      "Train loss: 0.542047\t Valid acc: 0.732953\n",
      "Train loss: 0.537683\t Valid acc: 0.754882\n",
      "Train loss: 0.525496\t Valid acc: 0.732453\n",
      "Train loss: 0.536741\t Valid acc: 0.763705\n",
      "Train loss: 0.564835\t Valid acc: 0.702861\n",
      "Train loss: 0.542592\t Valid acc: 0.755742\n"
     ]
    }
   ],
   "source": [
    "train_samples = int(0.9 * len(dataset))\n",
    "valid_samples = len(dataset) - train_samples\n",
    "train, valid = random_split(dataset, [train_samples, valid_samples])\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "model = CowCNN().cuda()\n",
    "lossfunc = tr.nn.CrossEntropyLoss()\n",
    "optimizer = tr.optim.Adam(model.parameters())\n",
    "\n",
    "best_valid_acc = 0\n",
    "epochs_without_improvement = 0\n",
    "while epochs_without_improvement < 20:\n",
    "\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for seq, lbl in train_loader:\n",
    "        seq, lbl = seq.cuda(), lbl.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        loss = lossfunc(model(seq), lbl.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "    \n",
    "    prediction, ground_truth = tr.LongTensor(), tr.LongTensor()\n",
    "    model.eval()\n",
    "    for seq, lbl in valid_loader:\n",
    "        seq, lbl = seq.cuda(), lbl.cuda()\n",
    "        prediction = tr.cat([prediction, tr.argmax(model(seq), 1).cpu()])\n",
    "        ground_truth = tr.cat([ground_truth, lbl.squeeze().cpu()])\n",
    "    valid_acc = balanced_accuracy_score(ground_truth.numpy(),\n",
    "                                        prediction.detach().numpy())\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        epochs_without_improvement = 0\n",
    "        tr.save(model.state_dict(), \"best_model.pmt\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        \n",
    "    print(\"Train loss: %f\\t Valid acc: %f\" % (train_loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hauvGV4hV-Mh"
   },
   "source": [
    "# Generar el archivo de predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zJKq2fLCDj-"
   },
   "source": [
    "Primero cargamos los parámetros del mejor modelo y lo ponemos en modo evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2N55NOf9yVL0"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(tr.load(\"best_model.pmt\"))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, ground_truth = tr.LongTensor(), tr.LongTensor()\n",
    "model.eval()\n",
    "for seq, lbl in valid_loader:\n",
    "    seq, lbl = seq.cuda(), lbl.cuda()\n",
    "    prediction = tr.cat([prediction, tr.argmax(model(seq), 1).cpu()])\n",
    "    ground_truth = tr.cat([ground_truth, lbl.squeeze().cpu()])\n",
    "valid_acc = balanced_accuracy_score(ground_truth.numpy(),\n",
    "                                    prediction.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvsEU37VCQYm"
   },
   "source": [
    "Cargamos los datos de test y realizamos predicciones sobre estos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9bKoUnty9KY"
   },
   "outputs": [],
   "source": [
    "dataset = WaveDataset(\"./data/\", \"test_files.csv\")\n",
    "test_loader = DataLoader(dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "pred = []\n",
    "for seq, _ in test_loader:\n",
    "        seq = seq.cuda()\n",
    "        batch_pred = tr.argmax(model(seq), dim=1)\n",
    "        pred.append(batch_pred.detach().cpu())\n",
    "pred = tr.cat(pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47Q7o-LBCVNn"
   },
   "source": [
    "Escribimos las predicciones junto a los ids en un archivo csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2E4EKhCWEC5"
   },
   "outputs": [],
   "source": [
    "ds = pd.DataFrame({'file' : dataset.filenames, 'prediction' : pred})\n",
    "ds.to_csv(\"predicciones.csv\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOHeSPaKq_F0"
   },
   "source": [
    "La siguiente celda debería descargar el archivo de predicciones. En caso de que falle volver a ejecutar el comando o descargarlo \"a mano\" en la solapa de archivos a la izquierda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZsfrcFBB6g1"
   },
   "outputs": [],
   "source": [
    "files.download('predicciones.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "u22w3BFiOveA",
    "RKKTU8QfXAEe",
    "Q3mrVb-RDVUU",
    "Ff1zx7-NYkXg"
   ],
   "name": "Copy of Competencia IA@Litoral2019 - Solución con aprendizaje profundo",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Pytorch (base env)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
